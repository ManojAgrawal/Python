{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy\n",
    "import nltk\n",
    "import nltk.data\n",
    "import collections\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "import os\n",
    "import sys\n",
    "import nltk.data\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/32879532/stanford-nlp-for-python\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "#corenlp = StanfordCoreNLP(corenlp_path=\"/Users/u606941/Downloads/stanford-corenlp-full-2017-06-09/\")\n",
    "sent_detector = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hardcoded word lists\n",
    "yesnowords = [\"can\", \"could\", \"would\", \"is\", \"does\", \"has\", \"was\", \"were\", \"had\", \"have\", \"did\", \"are\", \"will\",\"wa\"]\n",
    "commonwords = [\"the\", \"a\", \"an\", \"is\", \"are\", \"were\", \".\"]\n",
    "questionwords = [\"who\", \"what\", \"where\", \"when\", \"why\", \"how\", \"whose\", \"which\", \"whom\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take in a tokenized question and return the question type and body\n",
    "def processquestion(qwords):\n",
    "    \n",
    "    # Find \"question word\" (what, who, where, etc.)\n",
    "    questionword = \"\"\n",
    "    qidx = -1\n",
    "\n",
    "    for (idx, word) in enumerate(qwords):\n",
    "        if word.lower() in questionwords:\n",
    "            questionword = word.lower()\n",
    "            qidx = idx\n",
    "            break\n",
    "        elif word.lower() in yesnowords:\n",
    "            return (\"YESNO\", qwords)\n",
    "\n",
    "    if qidx < 0:\n",
    "        return (\"MISC\", qwords)\n",
    "\n",
    "    if qidx > len(qwords) - 3:\n",
    "        target = qwords[:qidx]\n",
    "    else:\n",
    "        target = qwords[qidx+1:]\n",
    "    type = \"MISC\"\n",
    "\n",
    "    # Determine question type\n",
    "    if questionword in [\"who\", \"whose\", \"whom\"]:\n",
    "        type = \"PERSON\"\n",
    "    elif questionword == \"where\":\n",
    "        type = \"PLACE\"\n",
    "    elif questionword == \"when\":\n",
    "        type = \"TIME\"\n",
    "    elif questionword == \"how\":\n",
    "        if target[0] in [\"few\", \"little\", \"much\", \"many\"]:\n",
    "            type = \"QUANTITY\"\n",
    "            target = target[1:]\n",
    "        elif target[0] in [\"young\", \"old\", \"long\"]:\n",
    "            type = \"TIME\"\n",
    "            target = target[1:]\n",
    "\n",
    "    # Trim possible extra helper verb\n",
    "    if questionword == \"which\":\n",
    "        target = target[1:]\n",
    "    if target[0] in yesnowords:\n",
    "        target = target[1:]\n",
    "    \n",
    "    # Return question data\n",
    "    return (type, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pdb\n",
    "def answeryesno(article, question):\n",
    "#    pdb.set_trace()\n",
    "    prev = \"no\"\n",
    "    questionstr = ' '.join(question)\n",
    "    questionstr = questionstr.lower()\n",
    "    question = nltk.pos_tag(question)\n",
    "    answer = \"no\"\n",
    "    keyword = \"\"\n",
    "    for (word,pos) in question:\n",
    "        if (pos == 'NN' or pos == 'NNS' or pos == 'NNP' or pos == 'NNPS'):\n",
    "            keyword = word.lower()\n",
    "            answer = \"no\"\n",
    "    for sentence in article:\n",
    "       # print sentence\n",
    "        if answer == \"yes\":\n",
    "            break\n",
    "        s = nltk.word_tokenize(sentence.lower())\n",
    "        if keyword in s:\n",
    "            #print sentence\n",
    "            answer = \"yes\"\n",
    "            for (word,pos) in question:\n",
    "                if answer == 'no': \n",
    "                    break\n",
    "                if (pos != '.') and (word.lower() not in s) and (pos != 'DT') and (word != 'does') and (word != 'do'):\n",
    "                    answer = 'no'                    \n",
    "                    #print word, pos\n",
    "                    if pos[0] == 'V':\n",
    "                        tempword = nltk.stem.wordnet.WordNetLemmatizer().lemmatize(word,'v')                       \n",
    "                        for (w,p) in nltk.pos_tag(s):\n",
    "                            if p[0] == 'V':\n",
    "                                tempword2 = nltk.stem.wordnet.WordNetLemmatizer().lemmatize(w,'v')\n",
    "                                if tempword == tempword2:\n",
    "                                    answer = 'yes'\n",
    "                    elif word in article[0]:                       \n",
    "                        answer = \"yes\"\n",
    "                if prev == \"yes\":\n",
    "                    if (word == \"no\" or word ==\"not\"):\n",
    "                        answer = \"no\"\n",
    "                if pos[0] == 'V':\n",
    "                    prev = \"yes\"\n",
    "                else:\n",
    "                    prev = \"no\"\n",
    "\n",
    "    #print questionstr,answer\n",
    "    return answer             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Process article file\n",
    "article = open(\"beatles.txt\", 'r')\n",
    "#article = 'John Lennon was a musician who founded The Beatles in 1960'\n",
    "article = BeautifulSoup(article, \"lxml\").get_text()\n",
    "article = ''.join([i if ord(i) < 128 else ' ' for i in article])\n",
    "article = article.replace(\"\\n\", \" . \")\n",
    "article = sent_detector.tokenize(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Process questions file\n",
    "#questions = open(questionsfilename, 'r').read()\n",
    "question = \"Was John Lennon shot?\"\n",
    "#questions = questions.decode('utf-8')\n",
    "#question = questions.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Was John Lennon shot?\n"
     ]
    }
   ],
   "source": [
    "# Answer not yet found\n",
    "done = False\n",
    "\n",
    "# Tokenize question\n",
    "print(question)\n",
    "qwords = nltk.word_tokenize(question.lower().replace('?', ''))\n",
    "questionPOS = nltk.pos_tag(qwords)\n",
    "qwords = [lemma.lemmatize(q) for q in qwords]\n",
    "\n",
    "# Process question\n",
    "(type, target) = processquestion(qwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'YESNO'"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer yes/no questions\n",
    "if type == \"YESNO\":\n",
    "    answer = answeryesno(article, qwords)\n",
    "    print(answer)\n",
    "#    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MISC'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sentence keywords\n",
    "searchwords = set(target).difference(commonwords)\n",
    "dict = collections.Counter()\n",
    "        \n",
    "# Find most relevant sentences\n",
    "for (i, sent) in enumerate(article):\n",
    "    sentwords = nltk.word_tokenize(sent.lower())\n",
    "    sentwords = [lemma.lemmatize(s) for s in sentwords]\n",
    "    wordmatches = set(filter(set(searchwords).__contains__, sentwords))\n",
    "    dict[sent] = len(wordmatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'john', 'lennon', 'shot', 'wa'}"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searchwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on 10 most relevant sentences\n",
    "import pdb\n",
    "answer = []\n",
    "for (sentence, matches) in dict.most_common(5):\n",
    "#    pdb.set_trace()\n",
    "    parse = nlp.annotate(sentence,\n",
    "                properties={\n",
    "                    'annotators': 'ner',\n",
    "                    'outputFormat': 'json',\n",
    "                    'timeout': 1000,\n",
    "                   })\n",
    "    sentencePOS = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "    done = False\n",
    "    # Attempt to find matching substrings\n",
    "    searchstring = ' '.join(target)\n",
    "    if searchstring in sentence.lower():\n",
    "#        startidx = sentence.lower().index(target[0])\n",
    "#        endidx = sentence.lower().index(target[-1])\n",
    "        answer.append(sentence)\n",
    "        done = True\n",
    "    \n",
    "    # Check if solution is found\n",
    "    if done:\n",
    "        continue\n",
    "\n",
    "    # Check by question type\n",
    "#    answer = \"\"\n",
    "#       for worddata in parse[\"sentences\"][0][\"words\"]:\n",
    "    for worddata in parse[\"sentences\"][0][\"tokens\"]:    \n",
    "        # Mentioned in the question\n",
    "#        if worddata[\"word\"] in searchwords:\n",
    "#            continue\n",
    "        if done == False:\n",
    "        \n",
    "            if type == \"PERSON\":\n",
    "                if worddata[\"ner\"] == \"PERSON\":\n",
    "                    answer.append(sentence)\n",
    "                    done = True\n",
    "#            elif done:\n",
    "#                break\n",
    "    # Check if solution is found\n",
    "            if done:\n",
    "                continue\n",
    "\n",
    "            if type == \"PLACE\":\n",
    "                if worddata[\"ner\"] == \"LOCATION\":\n",
    "                    answer.append(sentence)\n",
    "                    done = True\n",
    "#            elif done:\n",
    "#                break\n",
    "# Check if solution is found\n",
    "            if done:\n",
    "                continue\n",
    "\n",
    "            if type == \"QUANTITY\":\n",
    "                if worddata[\"ner\"] == \"NUMBER\":\n",
    "                    answer.append(sentence)\n",
    "                    done = True\n",
    "#            elif done:\n",
    "#                break\n",
    "# Check if solution is found\n",
    "            if done:\n",
    "                continue\n",
    "\n",
    "            if type == \"TIME\":\n",
    "                if worddata[\"ner\"] == \"NUMBER\":\n",
    "                    answer.append(sentence)\n",
    "                    done = True\n",
    "#            elif done:\n",
    "#                answer = sentence\n",
    "#                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unbeknownst to him, John Lennon and Paul McCartney were watching the live broadcast at John's apartment at The Dakota in New York at the time, which was within walking distance of the studio where the show was being shot.\n"
     ]
    }
   ],
   "source": [
    "if done:\n",
    "#    print(answer)\n",
    "    print(*answer, sep='\\n')\n",
    "if not done:\n",
    "    (answer, matches) = dict.most_common(1)[0]\n",
    "    print(answer)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
